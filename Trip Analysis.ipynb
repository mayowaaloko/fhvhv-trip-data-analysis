{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4fb3881-fb36-430e-8f70-a4673e2883f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Setting up the main output directory\n",
    "output_dir = \"/Workspace/Repos/mayowaaloko@gmail.com/fhvhv-trip-data-analysis\"\n",
    "visuals_dir = f\"{output_dir}/visualizations\"\n",
    "\n",
    "# Creating subdirectories for different types of analysis\n",
    "folders = [\n",
    "    f\"{visuals_dir}/01_cleaning\",\n",
    "    f\"{visuals_dir}/02_temporal\",\n",
    "    f\"{visuals_dir}/03_geographic\",\n",
    "    f\"{visuals_dir}/04_equity\",\n",
    "    f\"{visuals_dir}/05_environmental\",\n",
    "    f\"{visuals_dir}/06_economic\",\n",
    "    f\"{visuals_dir}/07_comparison\"\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    dbutils.fs.mkdirs(folder)\n",
    "    print(f\"‚úì Created: {folder}\")\n",
    "\n",
    "print(\"\\n Folder structure created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c3340c2-7a7d-419d-8e7c-e3d7319ca832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I'm importing all the tools I need for data processing and visualization\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Setting up my visualization preferences\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")\n",
    "print(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71bc4f2b-2325-4870-84c6-a32d48fb013b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I'm loading all three months of data from my Google Drive mount\n",
    "df_jan = spark.table(\"workspace.google_drive.fhvhv_tripdata_2025_01\")\n",
    "df_feb = spark.table(\"workspace.google_drive.fhvhv_tripdata_2025_02\")\n",
    "df_mar = spark.table(\"workspace.google_drive.fhvhv_tripdata_2025_03\")\n",
    "\n",
    "# Combining all three months into one dataset\n",
    "df_raw = df_jan.union(df_feb).union(df_mar)\n",
    "\n",
    "# Let me see what I'm working with\n",
    "print(f\"‚úÖ Data loaded successfully\")\n",
    "print(f\"Total records: {df_raw.count():,}\")\n",
    "print(f\"Total columns: {len(df_raw.columns)}\")\n",
    "\n",
    "# Saving the original count so I can track how much data I remove during cleaning\n",
    "original_count = df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c70209-cb20-4fa6-be10-9262c7f8ceee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I want to see the structure of my data before I start cleaning\n",
    "print(\"Column names and types:\")\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad3e3bec-be60-4af0-afcd-08ab4fee2f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Looking at a few sample records to understand the data\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df_raw.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6cd9e62-ef96-4e72-830d-b4c0addf90ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting a summary of what I'm dealing with\n",
    "print(f\"Dataset dimensions:\")\n",
    "print(f\"  Rows: {df_raw.count():,}\")\n",
    "print(f\"  Columns: {len(df_raw.columns)}\")\n",
    "\n",
    "# Showing all column names in a readable format\n",
    "print(f\"\\nAll columns ({len(df_raw.columns)} total):\")\n",
    "for i, col in enumerate(df_raw.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a6c254f-590c-4088-acac-dcb784e7722d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I need to know which columns have missing data and how much\n",
    "# This helps me decide what to drop vs what to keep\n",
    "\n",
    "missing_data = []\n",
    "\n",
    "for col in df_raw.columns:\n",
    "    null_count = df_raw.filter(F.col(col).isNull()).count()\n",
    "    null_pct = (null_count / original_count) * 100\n",
    "    \n",
    "    if null_count > 0:\n",
    "        missing_data.append({\n",
    "            'column': col,\n",
    "            'missing_count': null_count,\n",
    "            'missing_pct': round(null_pct, 2)\n",
    "        })\n",
    "\n",
    "# Converting to pandas for easier visualization\n",
    "missing_df = pd.DataFrame(missing_data).sort_values('missing_pct', ascending=False)\n",
    "\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f01247-5ff8-4809-8671-13b7a07a6839",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a chart to see which columns have the most missing data\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Only showing top 20 to keep it readable\n",
    "    top_missing = missing_df.head(20)\n",
    "    \n",
    "    sns.barplot(data=top_missing, y='column', x='missing_pct', ax=ax, palette='Reds_r')\n",
    "    ax.set_xlabel('Missing Percentage (%)', fontsize=12)\n",
    "    ax.set_ylabel('Column Name', fontsize=12)\n",
    "    ax.set_title('Top 20 Columns with Missing Data', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Adding percentage labels on bars\n",
    "    for i, v in enumerate(top_missing['missing_pct']):\n",
    "        ax.text(v + 0.5, i, f'{v:.1f}%', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚úÖ No missing data found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c191d9-090a-4d7c-badf-eda6c82c5fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I'm creating a copy to work with so I don't mess up the raw data\n",
    "df = df_raw\n",
    "\n",
    "print(\"‚úÖ Created working copy of data\")\n",
    "print(\"Starting data cleaning process...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab25048-b4d2-4bf2-a300-83ddb2a8028d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Making sure all datetime columns are properly formatted\n",
    "# These are critical for time-based analysis\n",
    "\n",
    "datetime_cols = ['request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime']\n",
    "\n",
    "print(\"Cleaning datetime columns...\")\n",
    "\n",
    "for col in datetime_cols:\n",
    "    if col in df.columns:\n",
    "        # Converting to proper timestamp format\n",
    "        df = df.withColumn(col, F.to_timestamp(F.col(col)))\n",
    "        print(f\"  ‚úì {col} converted to timestamp\")\n",
    "\n",
    "# Checking for any null values created during conversion\n",
    "print(\"\\nChecking for invalid dates:\")\n",
    "for col in datetime_cols:\n",
    "    if col in df.columns:\n",
    "        null_count = df.filter(F.col(col).isNull()).count()\n",
    "        if null_count > 0:\n",
    "            print(f\"  ‚ö†Ô∏è  {col}: {null_count:,} invalid dates\")\n",
    "        else:\n",
    "            print(f\"  ‚úì {col}: all valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eadd874c-8ae1-4bd9-bc1b-686c50b8e153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing records where dates don't make logical sense\n",
    "\n",
    "print(\"Removing records with invalid date logic...\")\n",
    "\n",
    "# Issue 1: Pickup should be after request\n",
    "before_count = df.count()\n",
    "df = df.filter(F.col('pickup_datetime') >= F.col('request_datetime'))\n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "print(f\"  Removed {removed:,} records where pickup was before request\")\n",
    "\n",
    "# Issue 2: Dropoff should be after pickup\n",
    "before_count = df.count()\n",
    "df = df.filter(F.col('dropoff_datetime') > F.col('pickup_datetime'))\n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "print(f\"  Removed {removed:,} records where dropoff was before/equal to pickup\")\n",
    "\n",
    "# Issue 3: All dates should be in Q1 2025\n",
    "before_count = df.count()\n",
    "df = df.filter(\n",
    "    (F.col('pickup_datetime') >= '2025-01-01') & \n",
    "    (F.col('pickup_datetime') < '2025-04-01')\n",
    ")\n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "print(f\"  Removed {removed:,} records outside Q1 2025\")\n",
    "\n",
    "print(f\"\\n‚úÖ Records remaining: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2d4ba5d-d5d6-4cdf-8e51-b766ba9bd1c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardizing the company identifiers and mapping to readable names\n",
    "\n",
    "print(\"Cleaning company license numbers...\")\n",
    "\n",
    "# Removing whitespace and making everything uppercase\n",
    "df = df.withColumn('hvfhs_license_num', F.upper(F.trim(F.col('hvfhs_license_num'))))\n",
    "\n",
    "# Showing the distribution\n",
    "print(\"\\nCompany distribution:\")\n",
    "df.groupBy('hvfhs_license_num').count().orderBy('count', ascending=False).show()\n",
    "\n",
    "# Mapping license numbers to company names\n",
    "# HV0003 = Uber, HV0005 = Lyft (official NYC TLC codes)\n",
    "df = df.withColumn('company_name',\n",
    "    F.when(F.col('hvfhs_license_num') == 'HV0003', 'Uber')\n",
    "     .when(F.col('hvfhs_license_num') == 'HV0005', 'Lyft')\n",
    "     .otherwise('Other')\n",
    ")\n",
    "\n",
    "print(\"\\nCompany names assigned:\")\n",
    "df.groupBy('company_name').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f4e6a5-13f5-4a5a-a301-32780c3a1b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These columns are from Databricks ingestion and not part of the actual NYC data\n",
    "# I'm dropping them because they don't help with the analysis\n",
    "\n",
    "print(\"Removing Databricks metadata columns...\")\n",
    "\n",
    "columns_to_drop = ['_line', '_fivetran_synced']\n",
    "\n",
    "for col in columns_to_drop:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(col)\n",
    "        print(f\"  ‚úì Dropped {col}\")\n",
    "\n",
    "print(f\"\\nColumns remaining: {len(df.columns)}\")\n",
    "print(f\"Records remaining: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac4b0e1-3d1b-4f0f-8611-65a5a4e4eb4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NYC has 263 official taxi zones (IDs 1-263)\n",
    "# Any ID outside this range is invalid\n",
    "\n",
    "print(\"Validating location IDs...\")\n",
    "\n",
    "# Checking pickup locations\n",
    "invalid_pickup = df.filter(\n",
    "    (F.col('pulocation_id') < 1) | \n",
    "    (F.col('pulocation_id') > 263) |\n",
    "    F.col('pulocation_id').isNull()\n",
    ").count()\n",
    "\n",
    "print(f\"  Invalid pickup locations: {invalid_pickup:,}\")\n",
    "\n",
    "# Checking dropoff locations\n",
    "invalid_dropoff = df.filter(\n",
    "    (F.col('dolocation_id') < 1) | \n",
    "    (F.col('dolocation_id') > 263) |\n",
    "    F.col('dolocation_id').isNull()\n",
    ").count()\n",
    "\n",
    "print(f\"  Invalid dropoff locations: {invalid_dropoff:,}\")\n",
    "\n",
    "# Removing records with invalid locations\n",
    "before_count = df.count()\n",
    "df = df.filter(\n",
    "    (F.col('pulocation_id').between(1, 263)) &\n",
    "    (F.col('dolocation_id').between(1, 263))\n",
    ")\n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "\n",
    "print(f\"  Removed {removed:,} records with invalid locations\")\n",
    "print(f\"\\n‚úÖ Records remaining: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e54ba25-fc31-44bd-be91-b653a39211ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I want to see which zones are most popular before cleaning further\n",
    "\n",
    "# Getting top 20 pickup zones\n",
    "top_pickups = df.groupBy('pulocation_id').count().orderBy('count', ascending=False).limit(20).toPandas()\n",
    "\n",
    "# Getting top 20 dropoff zones\n",
    "top_dropoffs = df.groupBy('dolocation_id').count().orderBy('count', ascending=False).limit(20).toPandas()\n",
    "\n",
    "# Creating side-by-side bar charts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Pickup zones chart\n",
    "axes[0].barh(range(len(top_pickups)), top_pickups['count'], color='steelblue')\n",
    "axes[0].set_yticks(range(len(top_pickups)))\n",
    "axes[0].set_yticklabels([f\"Zone {int(x)}\" for x in top_pickups['pulocation_id']])\n",
    "axes[0].set_xlabel('Number of Pickups')\n",
    "axes[0].set_title('Top 20 Pickup Locations', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Dropoff zones chart\n",
    "axes[1].barh(range(len(top_dropoffs)), top_dropoffs['count'], color='coral')\n",
    "axes[1].set_yticks(range(len(top_dropoffs)))\n",
    "axes[1].set_yticklabels([f\"Zone {int(x)}\" for x in top_dropoffs['dolocation_id']])\n",
    "axes[1].set_xlabel('Number of Dropoffs')\n",
    "axes[1].set_title('Top 20 Dropoff Locations', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "293b586f-89ff-4769-85b1-5d9a3d172fe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing trips with impossible distances\n",
    "\n",
    "print(\"Cleaning trip distances...\")\n",
    "\n",
    "# Checking current distribution\n",
    "print(\"\\nTrip distance statistics BEFORE cleaning:\")\n",
    "df.select('trip_miles').summary('count', 'mean', 'min', 'max', '50%', '75%', '95%').show()\n",
    "\n",
    "# Issue 1: Negative distances (impossible)\n",
    "negative_distance = df.filter(F.col('trip_miles') < 0).count()\n",
    "print(f\"  Negative distances: {negative_distance:,}\")\n",
    "\n",
    "# Issue 2: Zero distances (likely errors or cancellations)\n",
    "zero_distance = df.filter(F.col('trip_miles') == 0).count()\n",
    "print(f\"  Zero distances: {zero_distance:,}\")\n",
    "\n",
    "# Issue 3: Extremely long trips (over 100 miles likely errors)\n",
    "extreme_distance = df.filter(F.col('trip_miles') > 100).count()\n",
    "print(f\"  Trips over 100 miles: {extreme_distance:,}\")\n",
    "\n",
    "# Removing invalid distances\n",
    "before_count = df.count()\n",
    "df = df.filter(\n",
    "    (F.col('trip_miles') > 0) &\n",
    "    (F.col('trip_miles') <= 100)\n",
    ")\n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "\n",
    "print(f\"\\n  Removed {removed:,} records with invalid distances\")\n",
    "\n",
    "print(\"\\nTrip distance statistics AFTER cleaning:\")\n",
    "df.select('trip_miles').summary('count', 'mean', 'min', 'max', '50%', '75%', '95%').show()\n",
    "\n",
    "print(f\"‚úÖ Records remaining: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2de47b9-2738-40aa-b562-b4891a407e5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Removing trips with impossible durations\n",
    "\n",
    "print(\"Cleaning trip times...\")\n",
    "\n",
    "# Checking current distribution\n",
    "print(\"\\nTrip time statistics BEFORE cleaning:\")\n",
    "df.select('trip_time').summary('count', 'mean', 'min', 'max', '50%', '75%', '95%').show()\n",
    "\n",
    "# Issue 1: Negative or zero times (impossible)\n",
    "invalid_time = df.filter((F.col('trip_time') <= 0) | F.col('trip_time').isNull()).count()\n",
    "print(f\"  Zero or negative trip times: {invalid_time:,}\")\n",
    "\n",
    "# Issue 2: Extremely long trips (over 6 hours = 21600 seconds likely errors)\n",
    "extreme_time = df.filter(F.col('trip_time') > 21600).count()\n",
    "print(f\"  Trips over 6 hours: {extreme_time:,}\")\n",
    "\n",
    "# Removing invalid trip times\n",
    "before_count = df.count()\n",
    "df = df.filter(\n",
    "    (F.col('trip_time') > 0) &\n",
    "    (F.col('trip_time') <= 21600)\n",
    ")\n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "\n",
    "print(f\"\\n  Removed {removed:,} records with invalid trip times\")\n",
    "\n",
    "print(\"\\nTrip time statistics AFTER cleaning:\")\n",
    "df.select('trip_time').summary('count', 'mean', 'min', 'max', '50%', '75%', '95%').show()\n",
    "\n",
    "print(f\"‚úÖ Records remaining: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2541179c-26f8-4b21-8f8d-9f1ee51ea55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning all monetary fields and checking for logical consistency\n",
    "\n",
    "print(\"Cleaning fare components...\")\n",
    "\n",
    "fare_columns = ['base_passenger_fare', 'tolls', 'bcf', 'congestion_surcharge', \n",
    "                'sales_tax', 'tips', 'driver_pay', 'airport_fee', 'cbd_congestion_fee']\n",
    "\n",
    "# Checking each fare component\n",
    "print(\"\\nFare component statistics:\")\n",
    "for col in fare_columns:\n",
    "    if col in df.columns:\n",
    "        negative_count = df.filter(F.col(col) < 0).count()\n",
    "        null_count = df.filter(F.col(col).isNull()).count()\n",
    "        \n",
    "        if negative_count > 0 or null_count > 0:\n",
    "            print(f\"  {col}: {negative_count:,} negative | {null_count:,} null\")\n",
    "\n",
    "# Issue 1: Base fare must be positive (this is the core charge)\n",
    "before_count = df.count()\n",
    "df = df.filter(F.col('base_passenger_fare') > 0)\n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "print(f\"\\n  Removed {removed:,} records with invalid base fare\")\n",
    "\n",
    "# Issue 2: Tips can be null (means $0 tip) so I'll fill those with 0\n",
    "df = df.withColumn('tips', F.when(F.col('tips').isNull(), 0).otherwise(F.col('tips')))\n",
    "\n",
    "# Issue 3: Other fees that are null should also be 0\n",
    "for col in [\n",
    "    'tolls',\n",
    "    'bcf',\n",
    "    'congestion_surcharge',\n",
    "    'sales_tax',\n",
    "    'airport_fee',\n",
    "    'cbd_congestion_fee'\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df = df.withColumn(\n",
    "            col,\n",
    "            F.when(\n",
    "                F.col(col).isNull(),\n",
    "                0\n",
    "            ).otherwise(F.col(col))\n",
    "        )\n",
    "\n",
    "# Issue 4: Driver pay must be positive\n",
    "before_count = df.count()\n",
    "df = df.filter(F.col('driver_pay') > 0)\n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "print(f\"  Removed {removed:,} records with invalid driver pay\")\n",
    "\n",
    "print(f\"\\n‚úÖ Records remaining: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d76de72a-8d6e-4a9b-8cee-f9f19c9afe0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating distribution plots for key fare metrics\n",
    "\n",
    "# Getting a sample for visualization (plotting 60M points would crash)\n",
    "sample_df = df.sample(fraction=0.001, seed=42).select(\n",
    "    'base_passenger_fare', 'tips', 'driver_pay', 'trip_miles'\n",
    ").toPandas()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Base fare distribution\n",
    "axes[0, 0].hist(sample_df['base_passenger_fare'], bins=50, color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Base Passenger Fare ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Base Fare Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlim(0, 100)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tips distribution\n",
    "axes[0, 1].hist(sample_df['tips'], bins=50, color='green', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Tips ($)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Tips Distribution', fontweight='bold')\n",
    "axes[0, 1].set_xlim(0, 20)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Driver pay distribution\n",
    "axes[1, 0].hist(sample_df['driver_pay'], bins=50, color='coral', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Driver Pay ($)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Driver Pay Distribution', fontweight='bold')\n",
    "axes[1, 0].set_xlim(0, 80)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trip miles distribution\n",
    "axes[1, 1].hist(sample_df['trip_miles'], bins=50, color='purple', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Trip Distance (miles)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Trip Distance Distribution', fontweight='bold')\n",
    "axes[1, 1].set_xlim(0, 30)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Fare distributions look reasonable after cleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77ebc9b3-71c6-4c34-a940-d9d4f029a204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validating all the yes/no flag columns\n",
    "\n",
    "print(\"Cleaning flag fields...\")\n",
    "\n",
    "flag_columns = ['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', \n",
    "                'wav_request_flag', 'wav_match_flag']\n",
    "\n",
    "for col in flag_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col} distribution:\")\n",
    "        df.groupBy(col).count().orderBy('count', ascending=False).show()\n",
    "        \n",
    "        # These flags should only be Y, N, or null\n",
    "        # I'm standardizing them\n",
    "        df = df.withColumn(col, F.upper(F.trim(F.col(col))))\n",
    "        \n",
    "        # Converting nulls to 'N' (means the flag wasn't triggered)\n",
    "        df = df.withColumn(col, F.when(F.col(col).isNull(), 'N').otherwise(F.col(col)))\n",
    "        \n",
    "        print(f\"  ‚úì Standardized {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8373d05-1ac3-45b6-a407-4a6366b2f6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extracting useful time features from the pickup datetime\n",
    "\n",
    "print(\"Creating time-based features...\")\n",
    "\n",
    "df = df.withColumn('pickup_hour', F.hour('pickup_datetime'))\n",
    "df = df.withColumn('pickup_day_of_week', F.dayofweek('pickup_datetime'))  # 1=Sunday, 7=Saturday\n",
    "df = df.withColumn('pickup_date', F.to_date('pickup_datetime'))\n",
    "df = df.withColumn('pickup_month', F.month('pickup_datetime'))\n",
    "\n",
    "# Creating a weekend flag (Saturday=7, Sunday=1)\n",
    "df = df.withColumn('is_weekend', \n",
    "    F.when(F.col('pickup_day_of_week').isin([1, 7]), 'Y').otherwise('N')\n",
    ")\n",
    "\n",
    "# Creating day name for easier reading\n",
    "df = df.withColumn('day_name',\n",
    "    F.when(F.col('pickup_day_of_week') == 1, 'Sunday')\n",
    "     .when(F.col('pickup_day_of_week') == 2, 'Monday')\n",
    "     .when(F.col('pickup_day_of_week') == 3, 'Tuesday')\n",
    "     .when(F.col('pickup_day_of_week') == 4, 'Wednesday')\n",
    "     .when(F.col('pickup_day_of_week') == 5, 'Thursday')\n",
    "     .when(F.col('pickup_day_of_week') == 6, 'Friday')\n",
    "     .when(F.col('pickup_day_of_week') == 7, 'Saturday')\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Time features created:\")\n",
    "print(\"  - pickup_hour (0-23)\")\n",
    "print(\"  - pickup_day_of_week (1-7)\")\n",
    "print(\"  - pickup_date\")\n",
    "print(\"  - pickup_month (1-3)\")\n",
    "print(\"  - is_weekend (Y/N)\")\n",
    "print(\"  - day_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4819b8a7-4ac3-4859-ba9b-bf80402c4568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wait time is how long passengers waited between requesting and getting picked up\n",
    "# This is a key equity metric - longer waits indicate poor service\n",
    "\n",
    "print(\"Calculating wait time...\")\n",
    "\n",
    "df = df.withColumn('wait_time_seconds', \n",
    "    F.unix_timestamp('pickup_datetime') - F.unix_timestamp('request_datetime')\n",
    ")\n",
    "\n",
    "# Converting to minutes for easier interpretation\n",
    "df = df.withColumn('wait_time_minutes', F.col('wait_time_seconds') / 60)\n",
    "\n",
    "# Checking the distribution\n",
    "print(\"\\nWait time statistics:\")\n",
    "df.select('wait_time_minutes').summary('count', 'mean', 'min', 'max', '50%', '75%', '95%').show()\n",
    "\n",
    "# Removing extreme outliers (waits over 2 hours are likely data errors or abandoned requests)\n",
    "extreme_waits = df.filter(F.col('wait_time_minutes') > 120).count()\n",
    "print(f\"\\nWaits over 2 hours (likely errors): {extreme_waits:,}\")\n",
    "\n",
    "before_count = df.count()\n",
    "df = df.filter(F.col('wait_time_minutes') <= 120)\n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "\n",
    "print(f\"Removed {removed:,} records with extreme wait times\")\n",
    "print(f\"‚úÖ Records remaining: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd9067cb-d848-43e4-91b8-7b76c605aa8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This metric helps identify pricing inequality across different areas\n",
    "\n",
    "print(\"Calculating fare per mile...\")\n",
    "\n",
    "df = df.withColumn('fare_per_mile',\n",
    "    F.col('base_passenger_fare') / F.col('trip_miles')\n",
    ")\n",
    "\n",
    "# Checking the distribution\n",
    "print(\"\\nFare per mile statistics:\")\n",
    "df.select('fare_per_mile').summary('count', 'mean', 'min', 'max', '50%', '75%', '95%').show()\n",
    "\n",
    "# Removing extreme outliers (fare per mile over $100 is truly impossible in NYC)\n",
    "extreme_fpm = df.filter(F.col('fare_per_mile') > 100).count()\n",
    "print(f\"\\nFare per mile over $100: {extreme_fpm:,}\")\n",
    "\n",
    "before_count = df.count()\n",
    "df = df.filter(F.col('fare_per_mile') <= 100)   \n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "\n",
    "print(f\"Removed {removed:,} records with extreme fare per mile\")\n",
    "print(f\"Records remaining: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac98e5b7-3468-41be-9a11-1f2ad3d88596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This shows what percentage of the total fare goes to drivers vs the platform\n",
    "print(\"Calculating driver earnings percentage...\")\n",
    "\n",
    "# Total amount passenger paid (including tips ‚Äî this is correct for 2025 raw files)\n",
    "df = df.withColumn('passenger_total_paid',\n",
    "    F.col('base_passenger_fare') + \n",
    "    F.coalesce(F.col('tolls'), F.lit(0)) +\n",
    "    F.coalesce(F.col('bcf'), F.lit(0)) +\n",
    "    F.coalesce(F.col('congestion_surcharge'), F.lit(0)) +\n",
    "    F.coalesce(F.col('sales_tax'), F.lit(0)) +\n",
    "    F.coalesce(F.col('tips'), F.lit(0)) +\n",
    "    F.coalesce(F.col('airport_fee'), F.lit(0)) +\n",
    "    F.coalesce(F.col('cbd_congestion_fee'), F.lit(0))\n",
    ")\n",
    "\n",
    "# driver_pay √∑ total passenger paid = actual take-home %\n",
    "df = df.withColumn('driver_pct',\n",
    "    F.round(\n",
    "        F.when(F.col('passenger_total_paid') > 0,\n",
    "               (F.col('driver_pay') / F.col('passenger_total_paid') * 100)\n",
    "        ).otherwise(None),\n",
    "        2\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nDriver earnings percentage statistics:\")\n",
    "df.select('driver_pct').summary('count', 'mean', 'min', 'max', '50%', '75%', '95%').display()\n",
    "\n",
    "print(f\"\\nQ1 2025 average driver take-rate: {df.select(F.mean('driver_pct')).collect()[0][0]:.2f}%\")\n",
    "\n",
    "\n",
    "print(\"Driver percentage calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d92f5a3-ef98-4e68-8a47-f8652b7477bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using EPA standard: average car emits 0.411 kg CO‚ÇÇ per mile\n",
    "# This is for sustainability analysis\n",
    "\n",
    "print(\"Calculating CO‚ÇÇ emissions...\")\n",
    "\n",
    "CO2_PER_MILE = 0.411  # kg of CO‚ÇÇ per mile (EPA standard)\n",
    "\n",
    "df = df.withColumn('co2_kg', F.round(F.col('trip_miles') * F.lit(CO2_PER_MILE), 3))\n",
    "\n",
    "# Also converting to pounds for US audience\n",
    "df = df.withColumn('co2_lbs', F.round(F.col('co2_kg') * 2.20462, 3))\n",
    "\n",
    "print(\"\\nCO‚ÇÇ emissions statistics:\")\n",
    "df.select('co2_kg', 'co2_lbs').summary('count', 'mean', 'min', 'max', '50%', '75%', '95%').display()\n",
    "\n",
    "# Calculating total Q1 emissions\n",
    "total_co2_kg = df.agg(F.sum('co2_kg')).collect()[0][0]\n",
    "total_co2_tons = total_co2_kg / 1000\n",
    "\n",
    "print(f\"\\nüåç TOTAL Q1 2025 EMISSIONS:\")\n",
    "print(f\"   {total_co2_kg:,.0f} kg\")\n",
    "print(f\"   {total_co2_tons:,.0f} metric tons\")\n",
    "print(f\"   Equivalent to {total_co2_tons/4.6:,.0f} passenger vehicles driven for a year\")\n",
    "\n",
    "print(\"\\n‚úÖ Emissions calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14e92896-70b4-4d53-ba78-9b329501831e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b38b0e-2af3-4ca3-99ca-d64914a4a382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking if trip speeds make sense (helps catch remaining data errors)\n",
    "print(\"Calculating average speed...\")\n",
    "\n",
    "# Speed = distance / time\n",
    "# Trip time is in seconds ‚Üí convert to hours for mph\n",
    "df = df.withColumn('speed_mph',\n",
    "    F.round(\n",
    "        F.col('trip_miles') / (F.col('trip_time') / 3600), \n",
    "        2\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nSpeed statistics:\")\n",
    "df.select('speed_mph').summary('count', 'mean', 'min', 'max', '50%', '75%', '95%').display()\n",
    "\n",
    "# Checking for unrealistic speeds\n",
    "very_slow = df.filter(F.col('speed_mph') < 1).count()\n",
    "very_fast = df.filter(F.col('speed_mph') > 80).count()\n",
    "\n",
    "print(f\"\\nTrips under 1 mph (stuck in traffic/error): {very_slow:,}\")\n",
    "print(f\"Trips over 80 mph (likely errors): {very_fast:,}\")\n",
    "\n",
    "# Removing impossible speeds (over 80 mph in NYC is unrealistic)\n",
    "before_count = df.count()\n",
    "df = df.filter((F.col('speed_mph') >= 1) & (F.col('speed_mph') <= 80))\n",
    "after_count = df.count()\n",
    "removed = before_count - after_count\n",
    "\n",
    "print(f\"\\nRemoved {removed:,} records with unrealistic speeds\")\n",
    "print(f\"Records remaining: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b569dcd8-a281-4f25-9eff-d617026a3fb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Looking at how demand varies by hour of day\n",
    "print(\"Analyzing hourly patterns...\")\n",
    "\n",
    "# Getting hourly trip counts\n",
    "hourly_data = df.groupBy('pickup_hour').agg(\n",
    "    F.count('*').alias('trip_count'),\n",
    "    F.avg('wait_time_minutes').alias('avg_wait'),\n",
    "    F.avg('fare_per_mile').alias('avg_fare_per_mile'),\n",
    "    F.avg('driver_pct').alias('avg_driver_take_rate')   # ‚Üê how much drivers actually keep by hour\n",
    ").orderBy('pickup_hour').toPandas()\n",
    "\n",
    "# Creating a FOUR-panel visualization\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 16))\n",
    "\n",
    "# Panel 1: Trip volume by hour\n",
    "axes[0].bar(hourly_data['pickup_hour'], hourly_data['trip_count'], color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Hour of Day', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Trips', fontsize=12)\n",
    "axes[0].set_title('Hourly Trip Volume - Q1 2025', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(range(0, 24))\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 2: Average wait time by hour\n",
    "axes[1].plot(hourly_data['pickup_hour'], hourly_data['avg_wait'], marker='o', color='coral', linewidth=2)\n",
    "axes[1].set_xlabel('Hour of Day', fontsize=12)\n",
    "axes[1].set_ylabel('Average Wait Time (minutes)', fontsize=12)\n",
    "axes[1].set_title('Average Wait Time by Hour', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(range(0, 24))\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Average fare per mile by hour\n",
    "axes[2].plot(hourly_data['pickup_hour'], hourly_data['avg_fare_per_mile'], marker='s', color='green', linewidth=2)\n",
    "axes[2].set_xlabel('Hour of Day', fontsize=12)\n",
    "axes[2].set_ylabel('Fare per Mile ($)', fontsize=12)\n",
    "axes[2].set_title('Pricing by Hour', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xticks(range(0, 24))\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 4: Driver take-rate by hour (the money shot!)\n",
    "axes[3].plot(hourly_data['pickup_hour'], hourly_data['avg_driver_take_rate'], \n",
    "             marker='D', color='purple', linewidth=3, markersize=6)\n",
    "axes[3].set_xlabel('Hour of Day', fontsize=12)\n",
    "axes[3].set_ylabel('Driver Take Rate (%)', fontsize=12)\n",
    "axes[3].set_title('How Much Drivers Actually Keep by Hour', fontsize=14, fontweight='bold')\n",
    "axes[3].set_xticks(range(0, 24))\n",
    "axes[3].set_ylim(60, 90)\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "display(plt.gcf())\n",
    "plt.savefig(\n",
    "    '/Workspace/Repos/mayowaaloko@gmail.com/fhvhv-trip-data-analysis/visualizations/02_temporal/hourly_patterns.png',\n",
    "    dpi=300,\n",
    "    bbox_inches='tight',\n",
    "    facecolor='white'\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "print(\"Hourly patterns visualized ‚Äì including driver earnings percentage!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56454d40-6f7d-49fa-88c7-7abdab4f4675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Comparing weekday vs weekend patterns\n",
    "print(\"Analyzing day of week patterns...\")\n",
    "\n",
    "# Getting daily statistics\n",
    "daily_data = df.groupBy('day_name', 'pickup_day_of_week').agg(\n",
    "    F.count('*').alias('trip_count'),\n",
    "    F.avg('wait_time_minutes').alias('avg_wait'),\n",
    "    F.avg('base_passenger_fare').alias('avg_fare')\n",
    ").orderBy('pickup_day_of_week').toPandas()\n",
    "\n",
    "# Creating visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Panel 1: Trip volume by day\n",
    "axes[0].bar(daily_data['day_name'], daily_data['trip_count'], color='purple', edgecolor='black')\n",
    "axes[0].set_xlabel('Day of Week', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Trips', fontsize=12)\n",
    "axes[0].set_title('Trip Volume by Day', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 2: Wait times by day\n",
    "axes[1].bar(daily_data['day_name'], daily_data['avg_wait'], color='orange', edgecolor='black')\n",
    "axes[1].set_xlabel('Day of Week', fontsize=12)\n",
    "axes[1].set_ylabel('Average Wait Time (minutes)', fontsize=12)\n",
    "axes[1].set_title('Wait Times by Day', fontsize=14, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 3: Fares by day\n",
    "axes[2].bar(daily_data['day_name'], daily_data['avg_fare'], color='teal', edgecolor='black')\n",
    "axes[2].set_xlabel('Day of Week', fontsize=12)\n",
    "axes[2].set_ylabel('Average Fare ($)', fontsize=12)\n",
    "axes[2].set_title('Fares by Day', fontsize=14, fontweight='bold')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "display(plt.gcf()) \n",
    "plt.savefig(\n",
    "    '/Workspace/Repos/mayowaaloko@gmail.com/fhvhv-trip-data-analysis/visualizations/02_temporal/daily_patterns.png',\n",
    "    dpi=300,\n",
    "    bbox_inches='tight',\n",
    "    facecolor='white'\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "print(\"Day of week patterns visualized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7610b014-2f3d-414f-860f-7883a7c8d237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is critical for the accessibility crisis investigation\n",
    "print(\"Analyzing wheelchair accessibility...\")\n",
    "\n",
    "# Calculating accessibility rates\n",
    "wav_stats = df.groupBy('wav_match_flag').count().toPandas()\n",
    "total_trips = df.count()\n",
    "print(\"\\nWheelchair accessible vehicle (WAV) matches:\")\n",
    "print(wav_stats)\n",
    "\n",
    "wav_yes = df.filter(F.col('wav_match_flag') == 'Y').count()\n",
    "wav_pct = (wav_yes / total_trips) * 100\n",
    "\n",
    "print(f\"\\nACCESSIBILITY CRISIS:\")\n",
    "print(f\" WAV-matched trips: {wav_yes:,}\")\n",
    "print(f\" Percentage: {wav_pct:.3f}%\")\n",
    "print(f\" This means only {wav_pct:.3f}% of rides accommodate wheelchair users\")\n",
    "\n",
    "# Comparing WAV requests vs matches\n",
    "wav_requests = df.filter(F.col('wav_request_flag') == 'Y').count()\n",
    "wav_matches = df.filter((F.col('wav_request_flag') == 'Y') & (F.col('wav_match_flag') == 'Y')).count()\n",
    "match_rate = (wav_matches / wav_requests) * 100 if wav_requests > 0 else 0\n",
    "\n",
    "print(f\"\\n WAV requests: {wav_requests:,}\")\n",
    "print(f\" WAV matches (when requested): {wav_matches:,}\")\n",
    "print(f\" Match rate: {match_rate:.1f}%\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Left: overall WAV usage\n",
    "axes[0].pie([total_trips - wav_yes, wav_yes],\n",
    "            labels=['Not Accessible', 'Accessible'],\n",
    "            autopct='%1.3f%%',\n",
    "            colors=['#ff6b6b', '#51cf66'],\n",
    "            explode=(0, 0.1),\n",
    "            textprops={'fontsize': 13, 'weight': 'bold'},\n",
    "            shadow=True,\n",
    "            startangle=90)\n",
    "axes[0].set_title('Wheelchair Accessibility Rate', fontsize=15, fontweight='bold')\n",
    "\n",
    "# Right: fulfillment rate\n",
    "if wav_requests > 0:\n",
    "    axes[1].pie([wav_requests - wav_matches, wav_matches],\n",
    "                labels=['Unfulfilled', 'Fulfilled'],\n",
    "                autopct='%1.1f%%',\n",
    "                colors=['#ff6b6b', '#51cf66'],\n",
    "                explode=(0, 0.1),\n",
    "                textprops={'fontsize': 13, 'weight': 'bold'},\n",
    "                shadow=True,\n",
    "                startangle=90)\n",
    "else:\n",
    "    axes[1].pie([1], colors=['lightgray'])\n",
    "    axes[1].text(0, 0, 'No WAV\\nrequests', ha='center', va='center', fontsize=14, weight='bold')\n",
    "\n",
    "axes[1].set_title('WAV Request Fulfillment Rate', fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "display(plt.gcf())\n",
    "plt.savefig(\n",
    "    '/Workspace/Repos/mayowaaloko@gmail.com/fhvhv-trip-data-analysis/visualizations/04_equity/accessibility_crisis.png',\n",
    "    dpi=300,\n",
    "    bbox_inches='tight',\n",
    "    facecolor='white'\n",
    ")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAccessibility analysis complete\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Adding contextual visualization to clarify accessibility interpretation\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nAdding contextual accessibility chart...\")\n",
    "\n",
    "# Calculating proportions for added context\n",
    "non_wav_trips = total_trips - wav_yes\n",
    "\n",
    "fig2, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Bar chart showing:\n",
    "# 1. Total trips\n",
    "# 2. WAV requests\n",
    "# 3. WAV matches\n",
    "ax.bar(['Total Trips', 'WAV Requests', 'WAV Matches'],\n",
    "       [total_trips, wav_requests, wav_matches],\n",
    "       color=['#4dabf7', '#ffa94d', '#51cf66'])\n",
    "\n",
    "# Labeling bars with values\n",
    "for i, value in enumerate([total_trips, wav_requests, wav_matches]):\n",
    "    ax.text(i, value, f'{value:,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Title and explanation\n",
    "ax.set_title('Context: Comparing Total Trips vs WAV Requests vs Matches',\n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.set_ylabel('Count', fontsize=13)\n",
    "\n",
    "# Explanatory note directly in the plot\n",
    "ax.text(1, -max(total_trips, wav_requests, wav_matches) * 0.05,\n",
    "        \"Low WAV share (~9%) reflects low demand, not unmet need.\\n\"\n",
    "        \"Nearly all WAV requests are fulfilled (‚âà100%).\",\n",
    "        ha='center', va='top', fontsize=12)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig2)\n",
    "plt.close()\n",
    "\n",
    "print(\"Contextual visualization added.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a91fb9c-3d8b-4148-8c1f-cbb93fa63322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summarizing everything I've cleaned\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL CLEANING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_count = df.count()\n",
    "records_removed = original_count - final_count\n",
    "pct_removed = (records_removed / original_count) * 100\n",
    "\n",
    "print(f\"\\nDATA CLEANING RESULTS:\")\n",
    "print(f\"   Original records: {original_count:,}\")\n",
    "print(f\"   Final records: {final_count:,}\")\n",
    "print(f\"   Removed: {records_removed:,} ({pct_removed:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCLEANED DATASET READY:\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "print(f\"   Date range: Q1 2025 (Jan-Mar)\")\n",
    "print(f\"   Companies: Uber, Lyft\")\n",
    "print(f\"   All datetime fields validated\")\n",
    "print(f\"   All numeric fields validated\")\n",
    "print(f\"   All flags standardized\")\n",
    "print(f\"   New features created: 15\")\n",
    "\n",
    "print(\"\\nNEW FEATURES ADDED:\")\n",
    "new_features = [\n",
    "    'company_name', 'pickup_hour', 'pickup_day_of_week', 'pickup_date', \n",
    "    'pickup_month', 'is_weekend', 'day_name', 'wait_time_seconds', \n",
    "    'wait_time_minutes', 'fare_per_mile', 'total_fare', 'driver_pct', \n",
    "    'co2_kg', 'co2_lbs', 'speed_mph'\n",
    "]\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f41a946-432d-498f-a671-7b552a9a6eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving the cleaned dataset so I don't have to rerun cleaning every time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING CLEANED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Delta Lake is Databricks' optimized storage format\n",
    "# It's better than Parquet because it supports ACID transactions and time travel\n",
    "\n",
    "table_name = \"nyc_ridehail_q1_2025_clean\"\n",
    "\n",
    "print(f\"\\nSaving to Delta Lake table: {table_name}\")\n",
    "\n",
    "# Writing the cleaned data\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"‚úÖ Data saved successfully!\")\n",
    "\n",
    "# Verifying the save\n",
    "saved_count = spark.table(table_name).count()\n",
    "print(f\"\\nVerification: {saved_count:,} records in saved table\")\n",
    "\n",
    "if saved_count == final_count:\n",
    "    print(\"‚úì Record count matches!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: Record count mismatch\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: DATA CLEANING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55fa572-1c1a-4ff4-92b0-8f4a5412f08a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I'm creating a summary document of all the important findings from cleaning\n",
    "\n",
    "findings = {\n",
    "    \"Data Quality\": {\n",
    "        \"Original Records\": f\"{original_count:,}\",\n",
    "        \"Final Records\": f\"{final_count:,}\",\n",
    "        \"Records Removed\": f\"{original_count - final_count:,} ({((original_count - final_count)/original_count*100):.2f}%)\",\n",
    "        \"Data Quality\": \"High - only 5.33% removed\"\n",
    "    },\n",
    "    \n",
    "    \"Market Share\": {\n",
    "        \"Uber\": f\"{df.filter(F.col('company_name') == 'Uber').count():,} trips (77.04%)\",\n",
    "        \"Lyft\": f\"{df.filter(F.col('company_name') == 'Lyft').count():,} trips (22.96%)\"\n",
    "    },\n",
    "    \n",
    "    \"Temporal Patterns\": {\n",
    "        \"Peak Hour\": \"6 PM (18:00) - 3.4M trips\",\n",
    "        \"Lowest Hour\": \"4 AM - 850K trips\",\n",
    "        \"Busiest Day\": \"Saturday - 9.5M trips\",\n",
    "        \"Slowest Day\": \"Tuesday - 6.9M trips\",\n",
    "        \"Weekend Percentage\": f\"{(df.filter(F.col('is_weekend') == 'Y').count() / final_count * 100):.1f}%\"\n",
    "    },\n",
    "    \n",
    "    \"Service Metrics\": {\n",
    "        \"Average Wait Time\": \"4.67 minutes\",\n",
    "        \"Average Trip Distance\": \"4.37 miles\",\n",
    "        \"Average Trip Duration\": \"17.9 minutes\",\n",
    "        \"Average Speed\": \"13.2 mph\"\n",
    "    },\n",
    "    \n",
    "    \"Economic Metrics\": {\n",
    "        \"Average Base Fare\": f\"${df.agg(F.avg('base_passenger_fare')).collect()[0][0]:.2f}\",\n",
    "        \"Average Fare Per Mile\": f\"${df.agg(F.avg('fare_per_mile')).collect()[0][0]:.2f}\",\n",
    "        \"Average Driver Pay\": f\"${df.agg(F.avg('driver_pay')).collect()[0][0]:.2f}\",\n",
    "        \"Driver Take Rate\": \"61.74% of total fare\"\n",
    "    },\n",
    "    \n",
    "    \"Accessibility Crisis\": {\n",
    "        \"WAV Match Rate\": \"9.40% of all trips\",\n",
    "        \"WAV Requests\": \"135,829 (0.24% of trips)\",\n",
    "        \"Request Fulfillment\": \"~100% (nearly all requests matched)\",\n",
    "        \"Key Finding\": \"Low accessibility rate reflects low DEMAND, not unmet need\"\n",
    "    },\n",
    "    \n",
    "    \"Environmental Impact\": {\n",
    "        \"Total Q1 CO2 Emissions\": \"102,574 metric tons\",\n",
    "        \"Average Per Trip\": \"1.80 kg CO2\",\n",
    "        \"Equivalent To\": \"22,299 passenger vehicles for one year\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS FROM PHASE 1: DATA CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category, metrics in findings.items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Trip Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
